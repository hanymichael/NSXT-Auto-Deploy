##########################################################################################################################
## This is a sample configuration provided as a real-world example of how your vSphere/NSX-T configuration would look like. 
##########################################################################################################################

# vCenter Server Params 
vcip="192.168.110.20"
vchost="vcenter01.core.hypervizor.com"
vcadmin="administrator@core.hypervizor.com"
vcpass='VMware1!'
vcthumbprint="B8:43:29:5F:26:EA:C6:C9:27:57:AE:06:7A:42:30:AF:8D:DF:15:7F:B5:A7:19:4F:63:55:6E:70:6C:8B:F6:73"

# NSX-T Deployment Options 
nsxcluster="false"
nsxcomputemgr="true"
nsxvip="192.168.110.26"
mgrhostname="nsxtmgr"

# NSX-T Unified Appliance ((01)) Params
mgrname="NSX-T-Appliance01-$1-$2"
mgrdatastore="DatastoreCluster-CAI"
mgrnetwork="DPortGroup-CAI-VM-Management-Networks"
mgrip="192.168.110.17"
mgrnetmask="255.255.255.0"
mgrgw="192.168.110.2"
mgrdns="192.168.110.10"
mgrdomain="core.hypervizor.com"
mgrntp="192.168.110.2"
mgrssh="True"
mgrroot="True"
mgrformfactor="small"
mgrcorfu="CorfuDao: false"
mgrpasswd='VMware1!VMware1!'
mgrhostname01="nsxtmgr01"
mgrrole="NSX Manager"
mgresxhost="192.168.110.41"
ipAllocationPolicy="fixedPolicy"
logLevel="trivia"

# NSX-T Unified Appliance ((02)) Params
mgrname02="NSX-T-Appliance02-$1-$2"
mgrdatastore02="DatastoreCluster-CAI"
mgrnetwork02="DPortGroup-CAI-VM-Management-Networks"
mgrip02="192.168.110.18"
mgrnetmask02="255.255.255.0"
mgrgw02="192.168.110.2"
mgrdns02="192.168.110.10"
mgrdomain02="core.hypervizor.com"
mgrntp02="192.168.110.2"
mgrssh02="True"
mgrroot02="True"
mgrformfactor02="small"
mgrcorfu02="CorfuDao: false"
mgrpasswd02='VMware1!VMware1!'
mgrhostname02="nsxtmgr02"
mgrrole02="\"nsx-manager nsx-controller\""
mgresxhost02="192.168.110.42"
ipAllocationPolicy02="fixedPolicy"
logLevel02="trivia"

# NSX-T Unified Appliance ((03)) Params
mgrname03="NSX-T-Appliance03-$1-$2"
mgrdatastore03="DatastoreCluster-CAI"
mgrnetwork03="DPortGroup-CAI-VM-Management-Networks"
mgrip03="192.168.110.19"
mgrnetmask03="255.255.255.0"
mgrgw03="192.168.110.2"
mgrdns03="192.168.110.10"
mgrdomain03="core.hypervizor.com"
mgrntp03="192.168.110.2"
mgrssh03="True"
mgrroot03="True"
mgrformfactor03="small"
mgrcorfu03="CorfuDao: false"
mgrpasswd03='VMware1!VMware1!'
mgrhostname03="nsxtmgr03"
mgrrole03="\"nsx-manager nsx-controller\""
mgresxhost03="192.168.110.42"
ipAllocationPolicy03="fixedPolicy"
logLevel03="trivia"

# NSX-T Controller Params
ctrlname="NSX-T-Controller-$1-$2"
ctrldatastore="DatastoreCluster-CAI"
ctrlnetwork="DPortGroup-CAI-VM-Management-Networks"
ctrlip="192.168.110.29"
ctrlnetmask="255.255.255.0"
ctrlgw="192.168.110.2"
ctrldns="192.168.110.10"
ctrldomain="core.hypervizor.com"
ctrlntp="192.168.110.2"
ctrlssh="True"
ctrlroot="True"
ctrlpasswd='VMware1!VMware1!'
ctrlhostname="nsxtcont"
ctrlesxhost="192.168.110.42"
ctrlsecpasswd='VMware1!'

# NSX-T Edge01 Params
e1name="NSX-T-Edge01-$1-$2"
e1datastore="DatastoreCluster-CAI"
e1deploymentsize="medium"
e1net0="DPortGroup-CAI-VM-Management-Networks"
e1net1="DPortGroup-CAI-VM-Management-Networks"
e1net2="DPortGroup-CAI-Edge-Net01"
e1net3="ToR-CAI-Edge-Net-TRUNK-VLANs55-59"
e1ip="192.168.110.65"
e1netmask="255.255.255.0"
e1gw="192.168.110.2"
e1dns="192.168.110.10"
e1domain="core.hypervizor.com"
e1ntp="192.168.110.2"
e1ssh="True"
e1root="True"
e1passwd='VMware1!VMware1!'
e1hostname="nsxtedge01"
e1esxhost="192.168.110.43"

# NSX-T Edge02 Params
e2name="NSX-T-Edge02-$1-$2"
e2datastore="DatastoreCluster-CAI"
e2deploymentsize="medium"
e2net0="DPortGroup-CAI-VM-Management-Networks"
e2net1="DPortGroup-CAI-VM-Management-Networks"
e2net2="DPortGroup-CAI-Edge-Net01"
e2net3="ToR-CAI-Edge-Net-TRUNK-VLANs55-59"
e2ip="192.168.110.66"
e2netmask="255.255.255.0"
e2gw="192.168.110.2"
e2dns="192.168.110.10"
e2domain="core.hypervizor.com"
e2ntp="192.168.110.2"
e2ssh="True"
e2root="True"
e2passwd='VMware1!VMware1!'
e2hostname="nsxtedge02"
e2esxhost="192.168.110.43"

# NSX Buildweb URL - These URLs are masked. Contact me if you are a VMware employee to get the internal URLs
# For external use, login to myvmware, go to the NSX download area and paste the URL here.
nsxMgrBuildweb_URL="http://build-squid.eng.vmware.com/build/mts/release/bora-$2/publish/nsx-unified-appliance/exports/ova/"
nsxCtrlBuildweb_URL="http://build-squid.eng.vmware.com/build/mts/release/bora-$2/publish/nsx-controller/exports/ova/"
nsxEdgeBuildweb_URL="http://build-squid.eng.vmware.com/build/mts/release/bora-$2/publish/nsx-edgenode/exports/ova/"


# Tunnel End Point IP Pool
tep_display_name="TEP-IP-Pool"
tep_description="TEP IP Pool for Transport Nodes"
tep_dns_nameservers="192.168.110.10"
tep_start="192.168.110.180"
tep_end="192.168.110.189"
tep_gateway_ip="192.168.110.2"
tep_cidr="192.168.110.0/24"
tep_dns_suffix="core.hypervizor.com"

# Overlay Transport Zone
overlaytz_display_name="Overlay-TZ"
overlaytz_description="Overlay Transport Zone"
overlaytz_host_switch_name="NSXToverlay"

# VLAN Transport Zone
vlantz_display_name="VLAN-TZ"
vlantz_description="VLAN Transport Zone"
vlantz_host_switch_name="NSXTvlan"

# ESXi Host Nodes
esxihostnode01_display_name="ESXCNA01"
esxihostnode01_ip_addresses="192.168.110.81"
esxihostnode01_username="root"
esxihostnode01_password="VMware1!"
esxihostnode01_thumbprint="2C:C6:C0:D7:EE:88:B2:9C:48:E9:36:A6:29:FE:81:10:A0:C7:84:6C:82:BA:48:3A:D4:9F:E1:C2:1C:B2:2C:35"

esxihostnode02_display_name="ESXCNA02"
esxihostnode02_ip_addresses="192.168.110.82"
esxihostnode02_username="root"
esxihostnode02_password="VMware1!"
esxihostnode02_thumbprint="77:FE:11:F5:E5:0B:10:2A:16:92:59:AB:CF:DB:12:87:ED:40:C5:F2:9F:04:E4:AE:4D:1D:C4:E2:16:52:90:FC"

esxihostnode03_display_name="ESXCNA03"
esxihostnode03_ip_addresses="192.168.110.83"
esxihostnode03_username="root"
esxihostnode03_password="VMware1!"
esxihostnode03_thumbprint="68:34:52:CF:3C:6E:5F:55:99:A9:09:71:67:AC:1F:FA:81:61:64:85:4A:F6:7B:65:C5:55:90:7F:CB:B8:DA:9D"

# Edge Nodes
edgenode01_display_name="nsxtedge01"
edgenode01_description="Edge Node 01 located in Rack 01"

edgenode02_display_name="nsxtedge02"
edgenode02_description="Edge Node 02 located in Rack 01"

# Logical Switch for the Edge Uplink
logicalswitch_display_name="Cisco-CSR-VLAN-LS"
logicalswitch_vlan="0"

# ESXi host Uplink Profile
esxuplinkprofile_display_name="ESXi-Host-Profile"
esxuplinkprofile_mtu="1600"
esxuplinkprofile_uplink_name="uplink1"
esxuplinkprofile_policy="FAILOVER_ORDER"
esxuplinkprofile_transport_vlan="0"
esxuplinkprofile_device_name="vmnic2"

# Edge Node Uplinks Profile
edgeuplinkprofile1_display_name="NSX-T-Edge-Uplink1-Overlay-Profile"
edgeuplinkprofile1_mtu="1600"
edgeuplinkprofile1_uplink_name="uplink1"
edgeuplinkprofile1_policy="FAILOVER_ORDER"
edgeuplinkprofile1_transport_vlan="0"
edgeuplinkprofile1_device_name="fp-eth0"

edgeuplinkprofile2_display_name="NSX-T-Edge-Uplink2-CiscoCSR-Profile"
edgeuplinkprofile2_mtu="1600"
edgeuplinkprofile2_uplink_name="uplink2"
edgeuplinkprofile2_policy="FAILOVER_ORDER"
edgeuplinkprofile2_transport_vlan="0"
edgeuplinkprofile2_device_name="fp-eth1"

# Edge Cluster
edgecluster_display_name="EdgeCluster"
edgecluster_description="Edge Cluster for Cairo Site"

# Tier-0 Logical Router
t0router_display_name="CAI-Router-T0"
t0router_description="Tier-0 Router in Cairo Site"
t0router_high_availability_mode="ACTIVE_STANDBY"
t0router_preferred_edge_cluster_member_index="1"

# T0 Router Uplink (Logical Router Port)
t0_uplink_routerport_display_name="Edge01-Uplink-to-CiscoCSR"
t0_uplink_routerport_edge_cluster_member_index="[ 1 ]"
t0_uplink_routerport_ip_addresses="192.168.100.101"
t0_uplink_routerport_prefix_length="24"

# BGP Settings
bgp_as_num="65110"
bgp_remote_as_num="65100"
bgp_neighbor_display_name="CiscoCSR"
bgp_neighbor_address="192.168.100.2"

# Route Redistribution Settings
route_redist_display_name="RouteReDistro"
route_redist_sources=""


#################### The Tier-1 Logical Routers ######################

# Kubernetes on Ubuntu Tier-1 Router
kubeUbuntu_t1router_display_name="KubeUbuntu-Management-T1"
kubeUbuntu_mgmt_display_name="KubeUbuntu-Management-LS"
kubeUbuntu_t1router_description="Tier-1 Router for the Kubernetes on Ubuntu management networking"
kubeUbuntu_t1router_high_availability_mode="ACTIVE_STANDBY"
kubeUbuntu_subnet_gw="192.168.80.1"
kubeUbuntu_sugnet_prefix="24"

# Kubernetes on RHEL Tier-1 Router
kubeRHEL_t1router_display_name="KubeRHEL-Management-T1"
kubeRHEL_mgmt_display_name="KubeRHEL-Management-LS"
kubeRHEL_t1router_description="Tier-1 Router for the Kubernetes on RHEL management networking"
kubeRHEL_t1router_high_availability_mode="ACTIVE_STANDBY"
kubeRHEL_subnet_gw="192.168.60.1"
kubeRHEL_sugnet_prefix="24"

# Kubernetes on CentOS Tier-1 Router
kubeCentOS_t1router_display_name="KubeCentOS-Management-T1"
kubeCentOS_mgmt_display_name="KubeCentOS-Management-LS"
kubeCentOS_t1router_description="Tier-1 Router for the Kubernetes on CentOS management networking"
kubeCentOS_t1router_high_availability_mode="ACTIVE_STANDBY"
kubeCentOS_subnet_gw="192.168.50.1"
kubeCentOS_sugnet_prefix="24"

# OpenShift Tier-1 Router
os_t1router_display_name="OpenShift-Management-T1"
os_mgmt_display_name="OpenShift-Management-LS"
os_t1router_description="Tier-1 Router for the OpenShift management networking"
os_t1router_high_availability_mode="ACTIVE_STANDBY"
os_subnet_gw="192.168.90.1"
os_sugnet_prefix="24"

# IBM Cloud Private Tier-1 Router
ibm_t1router_display_name="IBM-CP-Management-T1"
ibm_mgmt_display_name="IBM-CP-Management-LS"
ibm_t1router_description="Tier-1 Router for the IBM Cloud private management networking"
ibm_t1router_high_availability_mode="ACTIVE_STANDBY"
ibm_subnet_gw="192.168.70.1"
ibm_sugnet_prefix="24"


######################################################################
##################### Container Network Plugin #######################
######################################################################

# NCP Clusters
kubeUbuntu_ncp_cluster_name="kubencp"
kubeRHEL_ncp_cluster_name="rhelncp"
kubeCentOS_ncp_cluster_name="centosncp"
openshift_ncp_cluster_name="osncp"
ibm_ncp_cluster_name="ibmncp"
pks_ncp_cluster_name="kpsncp"
vio_ncp_cluster_name="vioncp"

# KubeUbuntu NAT IP Pool
kubeUbuntu_ncp_display_name="K8s-NCP-NAT-IP-Pool"
kubeUbuntu_ncp_description="Kubernetes NCP IP Pool for External NAT Rules"
kubeUbuntu_ncp_dns_nameservers="192.168.110.10"
kubeUbuntu_ncp_start="192.168.100.120"
kubeUbuntu_ncp_end="192.168.100.139"
kubeUbuntu_ncp_gateway_ip="192.168.100.2"
kubeUbuntu_ncp_cidr="192.168.100.0/24"
kubeUbuntu_ncp_dns_suffix="core.hypervizor.com"

# KubeRHEL NAT IP Pool
kubeRHEL_ncp_display_name="K8sRHEL-NCP-NAT-IP-Pool"
kubeRHEL_ncp_description="Kubernetes on RHEL NCP IP Pool for External NAT Rules"
kubeRHEL_ncp_dns_nameservers="192.168.110.10"
kubeRHEL_ncp_start="192.168.100.140"
kubeRHEL_ncp_end="192.168.100.159"
kubeRHEL_ncp_gateway_ip="192.168.100.2"
kubeRHEL_ncp_cidr="192.168.100.0/24"
kubeRHEL_ncp_dns_suffix="core.hypervizor.com"

# KubeCentOS NAT IP Pool
kubeCentOS_ncp_display_name="K8sCentOS-NCP-NAT-IP-Pool"
kubeCentOS_ncp_description="Kubernetes on CentOS NCP IP Pool for External NAT Rules"
kubeCentOS_ncp_dns_nameservers="192.168.110.10"
kubeCentOS_ncp_start="192.168.100.160"
kubeCentOS_ncp_end="192.168.100.179"
kubeCentOS_ncp_gateway_ip="192.168.100.2"
kubeCentOS_ncp_cidr="192.168.100.0/24"
kubeCentOS_ncp_dns_suffix="core.hypervizor.com"

# OpenShift NAT IP Pool
os_ncp_display_name="OShift-NCP-NAT-IP-Pool"
os_ncp_description="RedHat OpenShift NCP IP Pool for External NAT Rules"
os_ncp_dns_nameservers="192.168.110.10"
os_ncp_start="192.168.100.180"
os_ncp_end="192.168.100.200"
os_ncp_gateway_ip="192.168.100.2"
os_ncp_cidr="192.168.100.0/24"
os_ncp_dns_suffix="core.hypervizor.com"

# IBM-CP NAT IP Pool
ibm_ncp_display_name="IBM-NCP-NAT-IP-Pool"
ibm_ncp_description="IBM Cloud private NCP IP Pool for External NAT Rules"
ibm_ncp_dns_nameservers="192.168.110.10"
ibm_ncp_start="192.168.100.201"
ibm_ncp_end="192.168.100.220"
ibm_ncp_gateway_ip="192.168.100.2"
ibm_ncp_cidr="192.168.100.0/24"
ibm_ncp_dns_suffix="core.hypervizor.com"

# KubeUbuntu Private IP Block
kubeUbuntu_ncp_block_display_name="KubeUbuntu-IP-Block"
kubeUbuntu_ncp_block_description="Kubernetes on Ubuntu Private IP Block"
kubeUbuntu_ncp_block_cidr="10.15.0.0/16"

# KubeRHEL Private IP Block
kubeRHEL_ncp_block_display_name="KubeRHEL-IP-Block"
kubeRHEL_ncp_block_description="Kubernetes on RHEL Private IP Block"
kubeRHEL_ncp_block_cidr="10.25.0.0/16"

# KubeCentOS Private IP Block
kubeCentOS_ncp_block_display_name="KubeCentOS-IP-Block"
kubeCentOS_ncp_block_description="Kubernetes on CentOS Private IP Block"
kubeCentOS_ncp_block_cidr="10.35.0.0/16"

# OpenShift Private IP Block
os_ncp_block_display_name="OShift-IP-Block"
os_ncp_block_description="OpenShift Private IP Block"
os_ncp_block_cidr="10.45.0.0/16"

# IBM-CP Private IP Block
ibm_ncp_block_display_name="IBM-IP-Block"
ibm_ncp_block_description="IBM CP Private IP Block"
ibm_ncp_block_cidr="10.55.0.0/16"

# Node Traffic Logical Switches
kubeUbuntu_nodes_traffic_ls="KubeUbuntu-Pods-LS"
kubeRHEL_nodes_traffic_ls="KubeRHEL-Pods-LS"
kubeCentOS_nodes_traffic_ls="KubeCentOS-Pods-LS"
os_nodes_traffic_ls="OpenShift-Pods-LS"
ibm_nodes_traffic_ls="IBM-Pods-LS"

# Master/Node VMs
kubeUbuntu_master_vm="vm-1585"
kubeUbuntu_node01_vm="vm-1586"
kubeUbuntu_node02_vm="vm-1587"
kubeUbuntu_master_name="kubemaster"
kubeUbuntu_node01_name="kubenode01"
kubeUbuntu_node02_name="kubenode02"
kubeUbuntu_master_hostname="kubemaster.core.hypervizor.com"
kubeUbuntu_node01_hostname="kubenode01.core.hypervizor.com"
kubeUbuntu_node02_hostname="kubenode02.core.hypervizor.com"

kubeRHEL_master_vm="vm-1642"
kubeRHEL_node01_vm="vm-1745"
kubeRHEL_node02_vm="vm-1746"
kubeRHEL_master_name="RedMaster"
kubeRHEL_node01_name="RedNode01"
kubeRHEL_node02_name="RedNode02"
kubeRHEL_master_hostname="RedMaster"
kubeRHEL_node01_hostname="RedNode01"
kubeRHEL_node02_hostname="RedNode02"

kubeCentOS_master_vm="vm-1742"
kubeCentOS_node01_vm="vm-1743"
kubeCentOS_node02_vm="vm-1744"
kubeCentOS_master_name="CentMaster"
kubeCentOS_node01_name="CentNode01"
kubeCentOS_node02_name="CentNode02"
kubeCentOS_master_hostname="CentMaster"
kubeCentOS_node01_hostname="CentNode01"
kubeCentOS_node02_hostname="CentNode02"

os_master_vm="vm-1589"
os_node01_vm="vm-1588"
os_node02_vm="vm-1590"
os_master_name="osmaster"
os_node01_name="osnode01"
os_node02_name="osnode02"
os_master_hostname="osmaster.core.hypervizor.com"
os_node01_hostname="osnode01.core.hypervizor.com"
os_node02_hostname="osnode02.core.hypervizor.com"

ibm_master_vm="vm-xxxx"
ibm_node01_vm="vm-xxxx"
ibm_node02_vm="vm-xxxx"
ibm_master_name="ibmmaster"
ibm_node01_name="ibmnode01"
ibm_node02_name="ibmnode02"
ibm_master_hostname="ibmmaster.core.hypervizor.com"
ibm_node01_hostname="ibmnode01.core.hypervizor.com"
ibm_node02_hostname="ibmnode02.core.hypervizor.com"

######################################################################
######################### Tasks & Progress ###########################
######################################################################
DeploymentTask01_Name="NSX-T Unified Appliance Download"
DeploymentTask01_Status=1
DeploymentTask02_Name="NSX-T Unified <<First>> Appliance Deployment"
DeploymentTask02_Status=1
DeploymentTask03_Name="NSX-T Unified <<Second>> Appliance Deployment"
DeploymentTask03_Status=0
DeploymentTask04_Name="NSX-T Unified <<Third>> Appliance Deployment"
DeploymentTask04_Status=0
DeploymentTask05_Name="NSX-T Edge Node Download"
DeploymentTask05_Status=1
DeploymentTask06_Name="NSX-T Edge Node 01 Deployment"
DeploymentTask06_Status=1
DeploymentTask07_Name="NSX-T Edge Node 02 Deployment"
DeploymentTask07_Status=1
DeploymentTask08_Name="Getting NSX-T Main Appliance's Cert. Thumbprint"
DeploymentTask08_Status=0
DeploymentTask09_Name="Joining Edge Node 01 to the NSX-T Manager"
DeploymentTask09_Status=1
DeploymentTask10_Name="Joining Edge Node 02 to the NSX-T Manager"
DeploymentTask10_Status=1
DeploymentTask11_Name="Adding vCenter as a Compute Manager"
DeploymentTask11_Status=1
DeploymentTask12_Name="Getting NSX-T Cluster ID"
DeploymentTask12_Status=0
DeploymentTask13_Name="Registering Second NSX-T Appliance Node"
DeploymentTask13_Status=0
DeploymentTask14_Name="Registering Third NSX-T Appliance Node"
DeploymentTask14_Status=0
DeploymentTask15_Name="Setting the Virtual IP"
DeploymentTask15_Status=0




